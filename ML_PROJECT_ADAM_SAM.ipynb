{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Modeling U.S Gun-Homcide Rates using SVI\n",
        "###A comprehensive analysis by Adam Sam\n",
        "\n",
        "<img src=\"https://www.bridgemi.com/sites/default/files/styles/full_width_image/public/2022-05/gun%20shutterstock.jpg?itok=AUziubj7\" width=\"520\" height=\"300\">\n",
        "\n"
      ],
      "metadata": {
        "id": "J0UngvoU0Bu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Datasets\n",
        "Gun homicide: https://wonder.cdc.gov/controller/saved/D158/D429F689\n",
        "\n",
        "SVI: https://www.atsdr.cdc.gov/place-health/php/svi/index.html\n",
        "\n",
        "**-Are there correlations between features?**\n",
        "\n",
        "The dataset includes multiple features grouped into four themes: Socioeconomic Status, Household Characteristics, Racial & Ethnic Minority Status, and Housing Type & Transportation.\n",
        "\n",
        "Features like poverty (EP_POV150),unemployment (EP_UNEMP), and lack of high school diploma (EP_NOHSDP) could likely be positively correlated.\n",
        "\n",
        "Features such as elderly population (EP_AGE65) and disability (EP_DISABL) may also be correlated.\n",
        "\n",
        "Variables starting with \"RPL_\" represent the final overall social vulnerability rankings calculated based on the preceding determining variables (non-RPL variables). These are likely correlated with said variables, and will be separated.\n",
        "\n",
        "A correlation matrix could quantitatively confirm these relationships.\n",
        "\n",
        "**-Are there outliers?**\n",
        "Washington D.C may be an outlier, as it has the highest homicide rate of 21 per 100k.\n",
        "\n",
        "Some other interesting outliers:\n",
        "EP_MINRTY (percentage who are considered minorities): Hawaii (73.76) and D.C (63.7) are much higher than the average.\n",
        "\n",
        "EP_UNINSUR (percentage who are uninsured): Texas (17.41) and Oklahoma (15.16) are outliers compared to states like Massachusetts (3.01).\n",
        "\n",
        "**-Will all features be used? Why or why not?**\n",
        "\n",
        "Not all features in the dataset were used, as some were deemed as adjunct, or irrelevant for measuring social vulnerability; particularly measures of ethnicity composition.\n",
        "\n",
        "The following variables are labeled as adjunct:\n",
        "\n",
        "-estimate of daytime population derived from LandScan 2021 estimates\n",
        "\n",
        "-ACS estimates for households without an internet subscription\n",
        "\n",
        "-ACS estimates for Hispanic/Latino persons, Not Hispanic or Latino Black/African American persons, Not Hispanic or Latino Asian persons, Not\n",
        "Hispanic or Latino American Indian and Alaska Native persons, Not Hispanic or Latino Native Hawaiian and Other Pacific Islander persons, Not Hispanic\n",
        "or Latino persons of two or more races, and Not Hispanic or Latino persons of some other race\n",
        "\n",
        "Variables that are not percentages were not used (raw values). Only the variables starting with \"EP_\" were used, as they represent percentages.\n",
        "\n",
        "Variables starting with \"RPL_\" represent the final overall social vulnerability rankings calculated based on the preceding determining variables (non-RPL variables). These are likely correlated with said varaibles, and will be separated.\n",
        "\n",
        "The RPL_THEMES feature is of main interest, as it is a ranking between 0 and 1 measuring social vulnerability considering all themes (socialeconomic, household, racial, transportation).\n",
        "\n",
        "RPL_THEME1 to RPL_THEME4 are separate rankings for individual layers of the SVI hierarchy.\n",
        "\n",
        "**-Is the dataset balanced?**\n",
        "The dataset appears balanced in terms of geographic representation, as it includes all U.S. states and counties with consistent metrics.\n",
        "\n",
        "However, the homicide rate per 100k varies widely (example: 1.28 in New Hampshire vs. 20.99 in D.C.), suggesting potential skewness. This could indicate imbalance if modeling homicide rates.\n",
        "\n",
        "**-What feature engineering is needed?**\n",
        "\n",
        "-Manually calculating homicide rate for rows where it is \"Unreliable.\"\n",
        "\n",
        "-Scaling the data using min=max scaling.\n",
        "\n",
        "-Dimensionality reduction using PCA\n",
        "\n",
        "**-Preprocessing steps**\n",
        "\n",
        "-Grouping the data by states\n",
        "\n",
        "-Replacing \"Unreliable\" values within death rate column with the proper values by manually calculating Deaths / Population * 100k.\n",
        "\n",
        "-Irrelevant columns were removed, including:\n",
        "\n",
        "*   Flags representing states fell within a certain category (0 or 1)\n",
        "*   Extraneous state codes and abbreviations\n",
        "*   Exact margin of error values\n",
        "*   Miscellaneous information not used in the measurement of SVI (ethnicity composition)"
      ],
      "metadata": {
        "id": "ZdIPxb7Y30_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DATASET LOADING"
      ],
      "metadata": {
        "id": "MGNR2nh2er_y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        },
        "id": "2oDMdhzMz-3L",
        "outputId": "8b581227-12d7-434a-b826-8f7ebadf6839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The memory_profiler extension is already loaded. To reload it, use:\n",
            "  %reload_ext memory_profiler\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   EP_POV150  EP_UNEMP   EP_HBURD  EP_NOHSDP  EP_UNINSUR   EP_AGE65  \\\n",
              "0  29.850746  5.905970  22.014925  15.489552    9.804478  19.061194   \n",
              "1  20.593333  7.893333  17.406667   8.780000   15.680000  14.456667   \n",
              "2  28.013333  7.146667  22.926667  13.900000   11.333333  21.880000   \n",
              "3  31.020000  5.973333  22.014667  13.812000    8.330667  19.936000   \n",
              "4  21.489655  6.765517  28.112069  14.094828    6.887931  18.608621   \n",
              "\n",
              "    EP_AGE17  EP_DISABL  EP_SNGPNT  EP_LIMENG  ...  EP_MOBILE   EP_CROWD  \\\n",
              "0  21.622388  19.011940   6.911940   0.822388  ...  21.141791   1.613433   \n",
              "1  23.646667  14.080000   6.016667   1.970000  ...   5.250000  10.753333   \n",
              "2  22.106667  16.086667   6.273333   4.686667  ...  20.173333   5.913333   \n",
              "3  22.144000  20.978667   6.780000   1.081333  ...  16.174667   2.529333   \n",
              "4  21.793103  13.675862   5.603448   6.181034  ...   7.115517   5.760345   \n",
              "\n",
              "    EP_NOVEH  EP_GROUPQ  RPL_THEME1  RPL_THEME2  RPL_THEME3  RPL_THEME4  \\\n",
              "0   6.574627   2.894030    0.693924    0.625413    0.674607    0.531418   \n",
              "1  21.553333   9.890000    0.478500    0.358120    0.832597    0.805073   \n",
              "2   5.826667   3.180000    0.749767    0.752953    0.839607    0.787700   \n",
              "3   6.849333   3.348000    0.673488    0.725617    0.560131    0.593488   \n",
              "4   5.698276   3.327586    0.631514    0.551059    0.802922    0.709186   \n",
              "\n",
              "   RPL_THEMES  Homicide Rate Per 100k  \n",
              "0    0.671582               12.297272  \n",
              "1    0.604770                5.589006  \n",
              "2    0.829213                6.658335  \n",
              "3    0.690255                9.521818  \n",
              "4    0.690778                4.294205  \n",
              "\n",
              "[5 rows x 22 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EP_POV150</th>\n",
              "      <th>EP_UNEMP</th>\n",
              "      <th>EP_HBURD</th>\n",
              "      <th>EP_NOHSDP</th>\n",
              "      <th>EP_UNINSUR</th>\n",
              "      <th>EP_AGE65</th>\n",
              "      <th>EP_AGE17</th>\n",
              "      <th>EP_DISABL</th>\n",
              "      <th>EP_SNGPNT</th>\n",
              "      <th>EP_LIMENG</th>\n",
              "      <th>...</th>\n",
              "      <th>EP_MOBILE</th>\n",
              "      <th>EP_CROWD</th>\n",
              "      <th>EP_NOVEH</th>\n",
              "      <th>EP_GROUPQ</th>\n",
              "      <th>RPL_THEME1</th>\n",
              "      <th>RPL_THEME2</th>\n",
              "      <th>RPL_THEME3</th>\n",
              "      <th>RPL_THEME4</th>\n",
              "      <th>RPL_THEMES</th>\n",
              "      <th>Homicide Rate Per 100k</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>29.850746</td>\n",
              "      <td>5.905970</td>\n",
              "      <td>22.014925</td>\n",
              "      <td>15.489552</td>\n",
              "      <td>9.804478</td>\n",
              "      <td>19.061194</td>\n",
              "      <td>21.622388</td>\n",
              "      <td>19.011940</td>\n",
              "      <td>6.911940</td>\n",
              "      <td>0.822388</td>\n",
              "      <td>...</td>\n",
              "      <td>21.141791</td>\n",
              "      <td>1.613433</td>\n",
              "      <td>6.574627</td>\n",
              "      <td>2.894030</td>\n",
              "      <td>0.693924</td>\n",
              "      <td>0.625413</td>\n",
              "      <td>0.674607</td>\n",
              "      <td>0.531418</td>\n",
              "      <td>0.671582</td>\n",
              "      <td>12.297272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.593333</td>\n",
              "      <td>7.893333</td>\n",
              "      <td>17.406667</td>\n",
              "      <td>8.780000</td>\n",
              "      <td>15.680000</td>\n",
              "      <td>14.456667</td>\n",
              "      <td>23.646667</td>\n",
              "      <td>14.080000</td>\n",
              "      <td>6.016667</td>\n",
              "      <td>1.970000</td>\n",
              "      <td>...</td>\n",
              "      <td>5.250000</td>\n",
              "      <td>10.753333</td>\n",
              "      <td>21.553333</td>\n",
              "      <td>9.890000</td>\n",
              "      <td>0.478500</td>\n",
              "      <td>0.358120</td>\n",
              "      <td>0.832597</td>\n",
              "      <td>0.805073</td>\n",
              "      <td>0.604770</td>\n",
              "      <td>5.589006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28.013333</td>\n",
              "      <td>7.146667</td>\n",
              "      <td>22.926667</td>\n",
              "      <td>13.900000</td>\n",
              "      <td>11.333333</td>\n",
              "      <td>21.880000</td>\n",
              "      <td>22.106667</td>\n",
              "      <td>16.086667</td>\n",
              "      <td>6.273333</td>\n",
              "      <td>4.686667</td>\n",
              "      <td>...</td>\n",
              "      <td>20.173333</td>\n",
              "      <td>5.913333</td>\n",
              "      <td>5.826667</td>\n",
              "      <td>3.180000</td>\n",
              "      <td>0.749767</td>\n",
              "      <td>0.752953</td>\n",
              "      <td>0.839607</td>\n",
              "      <td>0.787700</td>\n",
              "      <td>0.829213</td>\n",
              "      <td>6.658335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>31.020000</td>\n",
              "      <td>5.973333</td>\n",
              "      <td>22.014667</td>\n",
              "      <td>13.812000</td>\n",
              "      <td>8.330667</td>\n",
              "      <td>19.936000</td>\n",
              "      <td>22.144000</td>\n",
              "      <td>20.978667</td>\n",
              "      <td>6.780000</td>\n",
              "      <td>1.081333</td>\n",
              "      <td>...</td>\n",
              "      <td>16.174667</td>\n",
              "      <td>2.529333</td>\n",
              "      <td>6.849333</td>\n",
              "      <td>3.348000</td>\n",
              "      <td>0.673488</td>\n",
              "      <td>0.725617</td>\n",
              "      <td>0.560131</td>\n",
              "      <td>0.593488</td>\n",
              "      <td>0.690255</td>\n",
              "      <td>9.521818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21.489655</td>\n",
              "      <td>6.765517</td>\n",
              "      <td>28.112069</td>\n",
              "      <td>14.094828</td>\n",
              "      <td>6.887931</td>\n",
              "      <td>18.608621</td>\n",
              "      <td>21.793103</td>\n",
              "      <td>13.675862</td>\n",
              "      <td>5.603448</td>\n",
              "      <td>6.181034</td>\n",
              "      <td>...</td>\n",
              "      <td>7.115517</td>\n",
              "      <td>5.760345</td>\n",
              "      <td>5.698276</td>\n",
              "      <td>3.327586</td>\n",
              "      <td>0.631514</td>\n",
              "      <td>0.551059</td>\n",
              "      <td>0.802922</td>\n",
              "      <td>0.709186</td>\n",
              "      <td>0.690778</td>\n",
              "      <td>4.294205</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 22 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   EP_POV150  EP_UNEMP   EP_HBURD  EP_NOHSDP  EP_UNINSUR   EP_AGE65  \\\n",
              "0  29.850746  5.905970  22.014925  15.489552    9.804478  19.061194   \n",
              "1  20.593333  7.893333  17.406667   8.780000   15.680000  14.456667   \n",
              "2  28.013333  7.146667  22.926667  13.900000   11.333333  21.880000   \n",
              "3  31.020000  5.973333  22.014667  13.812000    8.330667  19.936000   \n",
              "4  21.489655  6.765517  28.112069  14.094828    6.887931  18.608621   \n",
              "\n",
              "    EP_AGE17  EP_DISABL  EP_SNGPNT  EP_LIMENG  ...  EP_MUNIT  EP_MOBILE  \\\n",
              "0  21.622388  19.011940   6.911940   0.822388  ...  3.277612  21.141791   \n",
              "1  23.646667  14.080000   6.016667   1.970000  ...  5.036667   5.250000   \n",
              "2  22.106667  16.086667   6.273333   4.686667  ...  4.800000  20.173333   \n",
              "3  22.144000  20.978667   6.780000   1.081333  ...  3.037333  16.174667   \n",
              "4  21.793103  13.675862   5.603448   6.181034  ...  9.300000   7.115517   \n",
              "\n",
              "    EP_CROWD   EP_NOVEH  EP_GROUPQ  RPL_THEME1  RPL_THEME2  RPL_THEME3  \\\n",
              "0   1.613433   6.574627   2.894030    0.693924    0.625413    0.674607   \n",
              "1  10.753333  21.553333   9.890000    0.478500    0.358120    0.832597   \n",
              "2   5.913333   5.826667   3.180000    0.749767    0.752953    0.839607   \n",
              "3   2.529333   6.849333   3.348000    0.673488    0.725617    0.560131   \n",
              "4   5.760345   5.698276   3.327586    0.631514    0.551059    0.802922   \n",
              "\n",
              "   RPL_THEME4  RPL_THEMES  \n",
              "0    0.531418    0.671582  \n",
              "1    0.805073    0.604770  \n",
              "2    0.787700    0.829213  \n",
              "3    0.593488    0.690255  \n",
              "4    0.709186    0.690778  \n",
              "\n",
              "[5 rows x 21 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EP_POV150</th>\n",
              "      <th>EP_UNEMP</th>\n",
              "      <th>EP_HBURD</th>\n",
              "      <th>EP_NOHSDP</th>\n",
              "      <th>EP_UNINSUR</th>\n",
              "      <th>EP_AGE65</th>\n",
              "      <th>EP_AGE17</th>\n",
              "      <th>EP_DISABL</th>\n",
              "      <th>EP_SNGPNT</th>\n",
              "      <th>EP_LIMENG</th>\n",
              "      <th>...</th>\n",
              "      <th>EP_MUNIT</th>\n",
              "      <th>EP_MOBILE</th>\n",
              "      <th>EP_CROWD</th>\n",
              "      <th>EP_NOVEH</th>\n",
              "      <th>EP_GROUPQ</th>\n",
              "      <th>RPL_THEME1</th>\n",
              "      <th>RPL_THEME2</th>\n",
              "      <th>RPL_THEME3</th>\n",
              "      <th>RPL_THEME4</th>\n",
              "      <th>RPL_THEMES</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>29.850746</td>\n",
              "      <td>5.905970</td>\n",
              "      <td>22.014925</td>\n",
              "      <td>15.489552</td>\n",
              "      <td>9.804478</td>\n",
              "      <td>19.061194</td>\n",
              "      <td>21.622388</td>\n",
              "      <td>19.011940</td>\n",
              "      <td>6.911940</td>\n",
              "      <td>0.822388</td>\n",
              "      <td>...</td>\n",
              "      <td>3.277612</td>\n",
              "      <td>21.141791</td>\n",
              "      <td>1.613433</td>\n",
              "      <td>6.574627</td>\n",
              "      <td>2.894030</td>\n",
              "      <td>0.693924</td>\n",
              "      <td>0.625413</td>\n",
              "      <td>0.674607</td>\n",
              "      <td>0.531418</td>\n",
              "      <td>0.671582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.593333</td>\n",
              "      <td>7.893333</td>\n",
              "      <td>17.406667</td>\n",
              "      <td>8.780000</td>\n",
              "      <td>15.680000</td>\n",
              "      <td>14.456667</td>\n",
              "      <td>23.646667</td>\n",
              "      <td>14.080000</td>\n",
              "      <td>6.016667</td>\n",
              "      <td>1.970000</td>\n",
              "      <td>...</td>\n",
              "      <td>5.036667</td>\n",
              "      <td>5.250000</td>\n",
              "      <td>10.753333</td>\n",
              "      <td>21.553333</td>\n",
              "      <td>9.890000</td>\n",
              "      <td>0.478500</td>\n",
              "      <td>0.358120</td>\n",
              "      <td>0.832597</td>\n",
              "      <td>0.805073</td>\n",
              "      <td>0.604770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28.013333</td>\n",
              "      <td>7.146667</td>\n",
              "      <td>22.926667</td>\n",
              "      <td>13.900000</td>\n",
              "      <td>11.333333</td>\n",
              "      <td>21.880000</td>\n",
              "      <td>22.106667</td>\n",
              "      <td>16.086667</td>\n",
              "      <td>6.273333</td>\n",
              "      <td>4.686667</td>\n",
              "      <td>...</td>\n",
              "      <td>4.800000</td>\n",
              "      <td>20.173333</td>\n",
              "      <td>5.913333</td>\n",
              "      <td>5.826667</td>\n",
              "      <td>3.180000</td>\n",
              "      <td>0.749767</td>\n",
              "      <td>0.752953</td>\n",
              "      <td>0.839607</td>\n",
              "      <td>0.787700</td>\n",
              "      <td>0.829213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>31.020000</td>\n",
              "      <td>5.973333</td>\n",
              "      <td>22.014667</td>\n",
              "      <td>13.812000</td>\n",
              "      <td>8.330667</td>\n",
              "      <td>19.936000</td>\n",
              "      <td>22.144000</td>\n",
              "      <td>20.978667</td>\n",
              "      <td>6.780000</td>\n",
              "      <td>1.081333</td>\n",
              "      <td>...</td>\n",
              "      <td>3.037333</td>\n",
              "      <td>16.174667</td>\n",
              "      <td>2.529333</td>\n",
              "      <td>6.849333</td>\n",
              "      <td>3.348000</td>\n",
              "      <td>0.673488</td>\n",
              "      <td>0.725617</td>\n",
              "      <td>0.560131</td>\n",
              "      <td>0.593488</td>\n",
              "      <td>0.690255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21.489655</td>\n",
              "      <td>6.765517</td>\n",
              "      <td>28.112069</td>\n",
              "      <td>14.094828</td>\n",
              "      <td>6.887931</td>\n",
              "      <td>18.608621</td>\n",
              "      <td>21.793103</td>\n",
              "      <td>13.675862</td>\n",
              "      <td>5.603448</td>\n",
              "      <td>6.181034</td>\n",
              "      <td>...</td>\n",
              "      <td>9.300000</td>\n",
              "      <td>7.115517</td>\n",
              "      <td>5.760345</td>\n",
              "      <td>5.698276</td>\n",
              "      <td>3.327586</td>\n",
              "      <td>0.631514</td>\n",
              "      <td>0.551059</td>\n",
              "      <td>0.802922</td>\n",
              "      <td>0.709186</td>\n",
              "      <td>0.690778</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   RPL_THEME1  RPL_THEME2  RPL_THEME3  RPL_THEME4  RPL_THEMES\n",
              "0    0.693924    0.625413    0.674607    0.531418    0.671582\n",
              "1    0.478500    0.358120    0.832597    0.805073    0.604770\n",
              "2    0.749767    0.752953    0.839607    0.787700    0.829213\n",
              "3    0.673488    0.725617    0.560131    0.593488    0.690255\n",
              "4    0.631514    0.551059    0.802922    0.709186    0.690778"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RPL_THEME1</th>\n",
              "      <th>RPL_THEME2</th>\n",
              "      <th>RPL_THEME3</th>\n",
              "      <th>RPL_THEME4</th>\n",
              "      <th>RPL_THEMES</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.693924</td>\n",
              "      <td>0.625413</td>\n",
              "      <td>0.674607</td>\n",
              "      <td>0.531418</td>\n",
              "      <td>0.671582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.478500</td>\n",
              "      <td>0.358120</td>\n",
              "      <td>0.832597</td>\n",
              "      <td>0.805073</td>\n",
              "      <td>0.604770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.749767</td>\n",
              "      <td>0.752953</td>\n",
              "      <td>0.839607</td>\n",
              "      <td>0.787700</td>\n",
              "      <td>0.829213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.673488</td>\n",
              "      <td>0.725617</td>\n",
              "      <td>0.560131</td>\n",
              "      <td>0.593488</td>\n",
              "      <td>0.690255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.631514</td>\n",
              "      <td>0.551059</td>\n",
              "      <td>0.802922</td>\n",
              "      <td>0.709186</td>\n",
              "      <td>0.690778</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   EP_POV150  EP_UNEMP   EP_HBURD  EP_NOHSDP  EP_UNINSUR   EP_AGE65  \\\n",
              "0  29.850746  5.905970  22.014925  15.489552    9.804478  19.061194   \n",
              "1  20.593333  7.893333  17.406667   8.780000   15.680000  14.456667   \n",
              "2  28.013333  7.146667  22.926667  13.900000   11.333333  21.880000   \n",
              "3  31.020000  5.973333  22.014667  13.812000    8.330667  19.936000   \n",
              "4  21.489655  6.765517  28.112069  14.094828    6.887931  18.608621   \n",
              "\n",
              "    EP_AGE17  EP_DISABL  EP_SNGPNT  EP_LIMENG  EP_MINRTY  EP_MUNIT  EP_MOBILE  \\\n",
              "0  21.622388  19.011940   6.911940   0.822388  36.023881  3.277612  21.141791   \n",
              "1  23.646667  14.080000   6.016667   1.970000  54.526667  5.036667   5.250000   \n",
              "2  22.106667  16.086667   6.273333   4.686667  50.940000  4.800000  20.173333   \n",
              "3  22.144000  20.978667   6.780000   1.081333  26.834667  3.037333  16.174667   \n",
              "4  21.793103  13.675862   5.603448   6.181034  48.439655  9.300000   7.115517   \n",
              "\n",
              "    EP_CROWD   EP_NOVEH  EP_GROUPQ  \n",
              "0   1.613433   6.574627   2.894030  \n",
              "1  10.753333  21.553333   9.890000  \n",
              "2   5.913333   5.826667   3.180000  \n",
              "3   2.529333   6.849333   3.348000  \n",
              "4   5.760345   5.698276   3.327586  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EP_POV150</th>\n",
              "      <th>EP_UNEMP</th>\n",
              "      <th>EP_HBURD</th>\n",
              "      <th>EP_NOHSDP</th>\n",
              "      <th>EP_UNINSUR</th>\n",
              "      <th>EP_AGE65</th>\n",
              "      <th>EP_AGE17</th>\n",
              "      <th>EP_DISABL</th>\n",
              "      <th>EP_SNGPNT</th>\n",
              "      <th>EP_LIMENG</th>\n",
              "      <th>EP_MINRTY</th>\n",
              "      <th>EP_MUNIT</th>\n",
              "      <th>EP_MOBILE</th>\n",
              "      <th>EP_CROWD</th>\n",
              "      <th>EP_NOVEH</th>\n",
              "      <th>EP_GROUPQ</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>29.850746</td>\n",
              "      <td>5.905970</td>\n",
              "      <td>22.014925</td>\n",
              "      <td>15.489552</td>\n",
              "      <td>9.804478</td>\n",
              "      <td>19.061194</td>\n",
              "      <td>21.622388</td>\n",
              "      <td>19.011940</td>\n",
              "      <td>6.911940</td>\n",
              "      <td>0.822388</td>\n",
              "      <td>36.023881</td>\n",
              "      <td>3.277612</td>\n",
              "      <td>21.141791</td>\n",
              "      <td>1.613433</td>\n",
              "      <td>6.574627</td>\n",
              "      <td>2.894030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.593333</td>\n",
              "      <td>7.893333</td>\n",
              "      <td>17.406667</td>\n",
              "      <td>8.780000</td>\n",
              "      <td>15.680000</td>\n",
              "      <td>14.456667</td>\n",
              "      <td>23.646667</td>\n",
              "      <td>14.080000</td>\n",
              "      <td>6.016667</td>\n",
              "      <td>1.970000</td>\n",
              "      <td>54.526667</td>\n",
              "      <td>5.036667</td>\n",
              "      <td>5.250000</td>\n",
              "      <td>10.753333</td>\n",
              "      <td>21.553333</td>\n",
              "      <td>9.890000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28.013333</td>\n",
              "      <td>7.146667</td>\n",
              "      <td>22.926667</td>\n",
              "      <td>13.900000</td>\n",
              "      <td>11.333333</td>\n",
              "      <td>21.880000</td>\n",
              "      <td>22.106667</td>\n",
              "      <td>16.086667</td>\n",
              "      <td>6.273333</td>\n",
              "      <td>4.686667</td>\n",
              "      <td>50.940000</td>\n",
              "      <td>4.800000</td>\n",
              "      <td>20.173333</td>\n",
              "      <td>5.913333</td>\n",
              "      <td>5.826667</td>\n",
              "      <td>3.180000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>31.020000</td>\n",
              "      <td>5.973333</td>\n",
              "      <td>22.014667</td>\n",
              "      <td>13.812000</td>\n",
              "      <td>8.330667</td>\n",
              "      <td>19.936000</td>\n",
              "      <td>22.144000</td>\n",
              "      <td>20.978667</td>\n",
              "      <td>6.780000</td>\n",
              "      <td>1.081333</td>\n",
              "      <td>26.834667</td>\n",
              "      <td>3.037333</td>\n",
              "      <td>16.174667</td>\n",
              "      <td>2.529333</td>\n",
              "      <td>6.849333</td>\n",
              "      <td>3.348000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21.489655</td>\n",
              "      <td>6.765517</td>\n",
              "      <td>28.112069</td>\n",
              "      <td>14.094828</td>\n",
              "      <td>6.887931</td>\n",
              "      <td>18.608621</td>\n",
              "      <td>21.793103</td>\n",
              "      <td>13.675862</td>\n",
              "      <td>5.603448</td>\n",
              "      <td>6.181034</td>\n",
              "      <td>48.439655</td>\n",
              "      <td>9.300000</td>\n",
              "      <td>7.115517</td>\n",
              "      <td>5.760345</td>\n",
              "      <td>5.698276</td>\n",
              "      <td>3.327586</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\"\"\" ADAM SAM\n",
        "steps to connect to lambda machine for co-lab\n",
        "1. ssh -L 8888:localhost:8888 sama1@lab.cs.wit.edu -p 50004\n",
        "\n",
        "2. jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com'\n",
        "--port=8888 --NotebookApp.port_retries=0\n",
        "\n",
        "3.\n",
        "On your PC, go to Google Colab and select \"Connect to a local runtime\" (top\n",
        "right corner).\n",
        "\n",
        "In the URL box, enter the URL with the token you received from the output of\n",
        "the Jupyter Notebook command in Step 2.\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%load_ext memory_profiler\n",
        "\n",
        "####MAKE SURE TO CD TO ML_PROJECT DIR FIRST!#####\n",
        "#!cd ML_PROJECT\n",
        "#scp -P 50004 \"C:\\Users\\sama1\\Downloads\\f2\\svi\\SVI_FINAL_CLEAN_2022.csv\" sama1@lab.cs.wit.edu:\"~/ML_PROJECT\"\n",
        "#loading dataset\n",
        "data = pd.read_csv(\"SVI_FINAL_CLEAN_2022.csv\")\n",
        "#data.head()\n",
        "\n",
        "#drop \"Deaths\" and \"Population\" and \"STATE\"\n",
        "data = data.drop([\"STATE\",\"Deaths\", \"Population\"], axis=1)\n",
        "display(data.head())\n",
        "\n",
        "target = data.iloc[:,-1]\n",
        "\n",
        "#display(target)\n",
        "\n",
        "data = data.drop(columns=data.columns[-1])\n",
        "display(data.head())\n",
        "#separate \"RPL_\" columns\n",
        "data_with_ranks = data.loc[:, data.columns.str.startswith('RPL_')]\n",
        "data = data.loc[:, ~data.columns.str.startswith('RPL_')]\n",
        "\n",
        "display(data_with_ranks.head())\n",
        "display(data.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MODEL SELECTION BASELINE"
      ],
      "metadata": {
        "id": "fzyaY19yeuYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import time,sys\n",
        "from io import StringIO\n",
        "\n",
        "#BASELINE training on unscaled data!\n",
        "#split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "#linear regression. NOTE: Linear regression has no hyperparameters, so no CV is needed\n",
        "lin_reg = LinearRegression()\n",
        "\n",
        "t0 = time.time()\n",
        "%memit lin_reg.fit(X_train, y_train)\n",
        "t1 = time.time()\n",
        "base_lin_train_time = t1 - t0\n",
        "\n",
        "t0 = time.time()\n",
        "base_y_pred_lin = lin_reg.predict(X_test)\n",
        "t1 = time.time()\n",
        "base_lin_infer_time = t1 - t0\n",
        "#SVR\n",
        "#Cross-validate SVR with GRIDSEARCH\n",
        "svr_param_grid = {'C': np.linspace(0.1, 10, 10), #higher C means less tolerance, lower means more tolerance\n",
        "              'kernel': ['linear', 'rbf', 'poly'],\n",
        "              'degree': [2, 3, 4,5,6],\n",
        "              'epsilon':np.linspace(0.001, 0.1, 10)}\n",
        "\n",
        "t0 = time.time()\n",
        "grid_search = GridSearchCV(SVR(), svr_param_grid, n_jobs=-1,cv=5, verbose=True, scoring = \"neg_root_mean_squared_error\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "t1 = time.time()\n",
        "base_svr_train_time = t1 - t0\n",
        "\n",
        "svr = grid_search.best_estimator_\n",
        "\n",
        "t0 = time.time()\n",
        "base_y_pred_svr = svr.predict(X_test)\n",
        "t1 = time.time()\n",
        "base_svr_infer_time = t1 - t0\n",
        "\n",
        "print(f\"Best SVR parameters: %s\" % grid_search.best_params_)\n",
        "\n",
        "#XGBoost\n",
        "\"\"\"\n",
        "n_estimators: The number of trees in the ensemble, often increased until no further improvements are seen.\n",
        "max_depth: The maximum depth of each tree, often values are between 1 and 10.\n",
        "eta: The learning rate used to weight each model, often set to small values such as 0.3, 0.1, 0.01, or smaller.\n",
        "subsample: The number of samples (rows) used in each tree, set to a value between 0 and 1, often 1.0 to use all samples.\n",
        "colsample_bytree: Number of features (columns) used in each tree, set to a value between 0 and 1, often 1.0 to use all features.\n",
        "\"\"\"\n",
        "XG_param_grid = {'n_estimators': [300,100,500],\n",
        "              'n_jobs':[-1],\n",
        "              'gamma': [0,1,1.5],\n",
        "              'max_depth': [5,8,12],\n",
        "              'eta': [0.1,0.3,0.05],\n",
        "              'subsample':[1],\n",
        "              'colsample_bytree':[0.75],\n",
        "              'reg_lambda':[0.5,1],\n",
        "              'reg_alpha':[3,2.5],\n",
        "              }\n",
        "\n",
        "grid_search = GridSearchCV(xgb.XGBRegressor(), XG_param_grid, cv=5, n_jobs=-1,scoring = \"neg_root_mean_squared_error\", verbose=True)\n",
        "t0 = time.time()\n",
        "grid_search = grid_search.fit(X_train, y_train)\n",
        "t1 = time.time()\n",
        "base_xgb_train_time = t1 - t0\n",
        "\n",
        "xgb_reg = grid_search.best_estimator_\n",
        "\n",
        "t0 = time.time()\n",
        "base_y_pred_xgb = xgb_reg.predict(X_test)\n",
        "t1= time.time()\n",
        "base_xgb_infer_time = t1 - t0\n",
        "\n",
        "print(f\"Best XGBoost parameters:%s\" % grid_search.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Riyrk_aYe0HO",
        "outputId": "29e43b75-086c-4b1d-89c4-edd287090df0"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "peak memory: 521.80 MiB, increment: 0.00 MiB\n",
            "Fitting 5 folds for each of 1500 candidates, totalling 7500 fits\n",
            "Best SVR parameters: {'C': 0.1, 'degree': 5, 'epsilon': 0.001, 'kernel': 'poly'}\n",
            "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
            "Best XGBoost parameters:{'colsample_bytree': 0.75, 'eta': 0.3, 'gamma': 1, 'max_depth': 5, 'n_estimators': 300, 'n_jobs': -1, 'reg_alpha': 3, 'reg_lambda': 0.5, 'subsample': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate baseline regression performances\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
        "#lin reg\n",
        "lin_reg_mse = mean_squared_error(y_test, base_y_pred_lin)\n",
        "lin_reg_r2 = r2_score(y_test, base_y_pred_lin)\n",
        "\n",
        "train_lin_reg_mse = mean_squared_error(y_train, lin_reg.predict(X_train))\n",
        "\n",
        "#svr\n",
        "svr_mse = mean_squared_error(y_test, base_y_pred_svr)\n",
        "svr_r2 = r2_score(y_test, base_y_pred_svr)\n",
        "\n",
        "train_svr_mse = mean_squared_error(y_train, svr.predict(X_train))\n",
        "\n",
        "\n",
        "#xgb\n",
        "xgb_mse = mean_squared_error(y_test, base_y_pred_xgb)\n",
        "xgb_r2 = r2_score(y_test, base_y_pred_xgb)\n",
        "\n",
        "train_xgb_mse = mean_squared_error(y_train, xgb_reg.predict(X_train))\n",
        "\n",
        "#print errors\n",
        "print(\"TRAINING VALIDATION\")\n",
        "print(f\"Linear Regression Training MSE: {train_lin_reg_mse}\")\n",
        "print(f\"SVR Training MSE: {train_svr_mse}\")\n",
        "print(f\"XGBoost Training MSE: {train_xgb_mse}\")\n",
        "\n",
        "print(\"TESTING VALIDATION\")\n",
        "print(f\"Linear Regression MSE: {lin_reg_mse}\")\n",
        "print(f\"Linear Regression R^2: {lin_reg_r2}\")\n",
        "print(f\"SVR MSE: {svr_mse}\")\n",
        "print(f\"SVR R^2: {svr_r2}\")\n",
        "print(f\"XGBoost MSE: {xgb_mse}\")\n",
        "print(f\"XGBoost R^2: {xgb_r2}\")\n",
        "\n",
        "#training times\n",
        "print(f\"Linear Regression Training Time: {base_lin_train_time} seconds\")\n",
        "print(f\"Base SVR Training Time: {base_svr_train_time} seconds\")\n",
        "print(f\"Base XGBoost Training Time: {base_xgb_train_time} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUCup5RZgT_X",
        "outputId": "2bc079d3-01c7-462f-f46c-a427b165df42"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING VALIDATION\n",
            "Linear Regression Training MSE: 1.8579327940238386\n",
            "SVR Training MSE: 3.3779118553893865\n",
            "XGBoost Training MSE: 0.9878889299335096\n",
            "TESTING VALIDATION\n",
            "Linear Regression MSE: 6.021257024000639\n",
            "Linear Regression R^2: 0.6646905299320609\n",
            "SVR MSE: 6.813620428609875\n",
            "SVR R^2: 0.6205656981499719\n",
            "XGBoost MSE: 5.910049667541645\n",
            "XGBoost R^2: 0.6708834028842183\n",
            "Linear Regression Training Time: 0.4662129878997803 seconds\n",
            "Base SVR Training Time: 4.946299314498901 seconds\n",
            "Base XGBoost Training Time: 2.923881769180298 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EXPERIMENT 1: FEATURE SCALING"
      ],
      "metadata": {
        "id": "Xh5Pk4eC6L8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize using minmax()\n",
        "#standard scaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
        "\n",
        "#SCALE ONLY THE FEATURES!\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "#TRAINING ON SCALED DATA!\n",
        "#linear regression. NOTE: Linear regression has no hyperparameters, so no CV is needed\n",
        "scale_lin_reg = LinearRegression()\n",
        "\n",
        "t0 = time.time()\n",
        "%memit scale_lin_reg.fit(X_train, y_train)\n",
        "t1 = time.time()\n",
        "scale_lin_train_time = t1 - t0\n",
        "\n",
        "t0 = time.time()\n",
        "scale_y_pred_lin = scale_lin_reg.predict(X_test)\n",
        "t1 = time.time()\n",
        "scale_lin_infer_time = t1 - t0\n",
        "\n",
        "#SVR\n",
        "grid_search = GridSearchCV(SVR(), svr_param_grid, cv=5, n_jobs=-1, verbose=True)\n",
        "\n",
        "t0 = time.time()\n",
        "%memit grid_search.fit(X_train, y_train)\n",
        "t1 = time.time()\n",
        "scale_svr_train_time = t1 - t0\n",
        "\n",
        "scale_svr = grid_search.best_estimator_\n",
        "\n",
        "t0 = time.time()\n",
        "scale_y_pred_svr = scale_svr.predict(X_test)\n",
        "t1 = time.time()\n",
        "scale_svr_infer_time = t1 - t0\n",
        "\n",
        "print(f\"Best SVR parameters: %s\" % grid_search.best_params_)\n",
        "\n",
        "#XGBoost\n",
        "\n",
        "#Best XGBoost parameters:{'colsample_bytree': 0.75, 'eta': 0.3, 'gamma': 1, 'max_depth': 5, 'n_estimators': 300, 'n_jobs': -1, 'reg_alpha': 3, 'reg_lambda': 0.5, 'subsample': 1}\n",
        "\n",
        "grid_search = GridSearchCV(xgb.XGBRegressor(), XG_param_grid, n_jobs=-1,cv=5, scoring = \"neg_root_mean_squared_error\", verbose=True)\n",
        "\n",
        "t0 = time.time()\n",
        "%memit grid_search.fit(X_train, y_train)\n",
        "t1 = time.time()\n",
        "scale_xgb_train_time = t1 - t0\n",
        "\n",
        "scale_xgb = grid_search.best_estimator_\n",
        "\n",
        "t0 = time.time()\n",
        "scale_y_pred_xgb = scale_xgb.predict(X_test)\n",
        "t1 = time.time()\n",
        "scale_xgb_infer_time = t1 - t0\n",
        "\n",
        "print(f\"Best XGBoost parameters:%s\" % grid_search.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91XxCj5X6JfI",
        "outputId": "b01f0b3a-2ded-4746-c357-81a12a751dd0"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "peak memory: 521.80 MiB, increment: 0.00 MiB\n",
            "Fitting 5 folds for each of 1500 candidates, totalling 7500 fits\n",
            "peak memory: 521.80 MiB, increment: 0.00 MiB\n",
            "Best SVR parameters: {'C': 2.3000000000000003, 'degree': 2, 'epsilon': 0.001, 'kernel': 'linear'}\n",
            "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
            "peak memory: 521.80 MiB, increment: 0.00 MiB\n",
            "Best XGBoost parameters:{'colsample_bytree': 0.75, 'eta': 0.3, 'gamma': 1, 'max_depth': 5, 'n_estimators': 300, 'n_jobs': -1, 'reg_alpha': 3, 'reg_lambda': 0.5, 'subsample': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate baseline regression performances\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
        "#lin reg\n",
        "scale_lin_reg_mse = mean_squared_error(y_test, scale_y_pred_lin)\n",
        "scale_lin_reg_r2 = r2_score(y_test, scale_y_pred_lin)\n",
        "train_scale_lin_reg_mse = mean_squared_error(y_train, scale_lin_reg.predict(X_train))\n",
        "\n",
        "\n",
        "#svr\n",
        "scale_svr_mse = mean_squared_error(y_test, scale_y_pred_svr)\n",
        "scale_svr_r2 = r2_score(y_test, scale_y_pred_svr)\n",
        "train_scale_svr_mse = mean_squared_error(y_train, scale_svr.predict(X_train))\n",
        "\n",
        "\n",
        "#xgb\n",
        "scale_xgb_mse = mean_squared_error(y_test, scale_y_pred_xgb)\n",
        "scale_xgb_r2 = r2_score(y_test, scale_y_pred_xgb)\n",
        "train_scale_xgb_mse = mean_squared_error(y_train, scale_xgb.predict(X_train))\n",
        "\n",
        "#print errors\n",
        "print(\"TRAINING VALIDATION\")\n",
        "print(f\"Linear Regression Training MSE: {train_scale_lin_reg_mse}\")\n",
        "print(f\"SVR Training MSE: {train_scale_svr_mse}\")\n",
        "print(f\"XGBoost Training MSE: {train_scale_xgb_mse}\")\n",
        "\n",
        "print(\"TESTING VALIDATION\")\n",
        "print(f\"Linear Regression MSE: {scale_lin_reg_mse}\")\n",
        "print(f\"Linear Regression R^2: {scale_lin_reg_r2}\")\n",
        "print(f\"SVR MSE: {scale_svr_mse}\")\n",
        "print(f\"SVR R^2: {scale_svr_r2}\")\n",
        "print(f\"XGBoost MSE: {scale_xgb_mse}\")\n",
        "print(f\"XGBoost R^2: {scale_xgb_r2}\")\n",
        "\n",
        "#training times\n",
        "print(f\"Scaled Linear Regression Training Time: {scale_lin_train_time} seconds\")\n",
        "print(f\"Scaled SVR Training Time: {scale_svr_train_time} seconds\")\n",
        "print(f\"Scaled XGBoost Training Time: {scale_xgb_train_time} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E11p5bMl-l-Z",
        "outputId": "f313ce38-ade7-4e3c-eed7-d8ba11aa40ec"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING VALIDATION\n",
            "Linear Regression Training MSE: 1.8579327940238386\n",
            "SVR Training MSE: 6.436057875193771\n",
            "XGBoost Training MSE: 0.9878889299335096\n",
            "TESTING VALIDATION\n",
            "Linear Regression MSE: 6.0212570240006364\n",
            "Linear Regression R^2: 0.6646905299320611\n",
            "SVR MSE: 5.340416141768769\n",
            "SVR R^2: 0.7026049379222508\n",
            "XGBoost MSE: 5.910049667541645\n",
            "XGBoost R^2: 0.6708834028842183\n",
            "Scaled Linear Regression Training Time: 0.4677116870880127 seconds\n",
            "Scaled SVR Training Time: 1.8613686561584473 seconds\n",
            "Scaled XGBoost Training Time: 2.731527090072632 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"OBSERVATION:\n",
        "Scaling the data appears to reduce training times with GridCV noticeably,\n",
        "splitting XGBoose training time in half.\n",
        "MSEs appears to be unchanged however, except for SVR, which improved.\n",
        "Also, SVR chose a linear kernel, rather than a polynomial one.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzfvHZCjUHss",
        "outputId": "4af72300-09e1-46ef-a75b-8d92ed016f55"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'OBSERVATION:\\nScaling the data appears to reduce training times with GridCV noticeably,\\nsplitting XGBoose training time in half.\\nMSEs appears to be unchanged however!\\nAlso, SVR choose a linear kernel, rather than a polynomial one.\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EXPERIMENT 2: Generate new features\n",
        "• Polynomial features (2nd and 3rd order)\n"
      ],
      "metadata": {
        "id": "kc4BjUAlBVMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2ND ORDER\n",
        "#generate polynomial features derived from X data\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
        "\n",
        "#SCALE ONLY THE FEATURES!\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "#degree=2 for 2nd order features\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "\n",
        "#fit and transform the features\n",
        "X_train = poly_features.fit_transform(X_train)\n",
        "X_test = poly_features.transform(X_test)\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "#linear regression\n",
        "poly_lin_reg = LinearRegression()\n",
        "\n",
        "t0 = time.time()\n",
        "%memit poly_lin_reg.fit(X_train, y_train)\n",
        "t1 = time.time()\n",
        "poly_lin_train_time = t1 - t0\n",
        "\n",
        "t0 = time.time()\n",
        "poly_y_pred_lin = poly_lin_reg.predict(X_test)\n",
        "t1 = time.time()\n",
        "poly_lin_infer_time = t1 - t0\n",
        "\n",
        "#SVR\n",
        "grid_search = GridSearchCV(SVR(), svr_param_grid, cv=5, n_jobs=-1,verbose=True)\n",
        "\n",
        "t0 = time.time()\n",
        "%memit grid_search.fit(X_train, y_train)\n",
        "t1 = time.time()\n",
        "poly_svr_train_time = t1 - t0\n",
        "\n",
        "poly_svr = grid_search.best_estimator_\n",
        "\n",
        "t0 = time.time()\n",
        "poly_y_pred_svr = poly_svr.predict(X_test)\n",
        "t1 = time.time()\n",
        "poly_svr_infer_time = t1 - t0\n",
        "\n",
        "print(f\"Best SVR parameters: %s\" % grid_search.best_params_)\n",
        "\n",
        "#XGBoost\n",
        "\n",
        "#Best XGBoost parameters:{'colsample_bytree': 0.75, 'eta': 0.3, 'gamma': 1, 'max_depth': 5, 'n_estimators': 300, 'n_jobs': -1, 'reg_alpha': 3, 'reg_lambda': 0.5, 'subsample': 1}\n",
        "\n",
        "grid_search = GridSearchCV(xgb.XGBRegressor(), XG_param_grid, cv=5, n_jobs=-1, scoring = \"neg_root_mean_squared_error\", verbose=True)\n",
        "\n",
        "t0 = time.time()\n",
        "%memit grid_search.fit(X_train, y_train)\n",
        "t1 = time.time()\n",
        "poly_xgb_train_time = t1 - t0\n",
        "\n",
        "poly_xgb = grid_search.best_estimator_\n",
        "\n",
        "t0 = time.time()\n",
        "poly_y_pred_xgb = poly_xgb.predict(X_test)\n",
        "t1 = time.time()\n",
        "poly_xgb_infer_time = t1 - t0\n",
        "\n",
        "print(f\"Best XGBoost parameters:%s\" % grid_search.best_params_)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIPUmwaQCtne",
        "outputId": "25b5800c-a69d-4eac-c667-6246ba5a3343"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(40, 16)\n",
            "(40, 152)\n",
            "peak memory: 522.05 MiB, increment: 0.25 MiB\n",
            "Fitting 5 folds for each of 1500 candidates, totalling 7500 fits\n",
            "peak memory: 522.05 MiB, increment: 0.00 MiB\n",
            "Best SVR parameters: {'C': 7.800000000000001, 'degree': 2, 'epsilon': 0.001, 'kernel': 'linear'}\n",
            "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
            "peak memory: 522.05 MiB, increment: 0.00 MiB\n",
            "Best XGBoost parameters:{'colsample_bytree': 0.75, 'eta': 0.1, 'gamma': 1.5, 'max_depth': 5, 'n_estimators': 300, 'n_jobs': -1, 'reg_alpha': 3, 'reg_lambda': 0.5, 'subsample': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate\n",
        "#lin reg\n",
        "poly_lin_reg_mse = mean_squared_error(y_test, poly_y_pred_lin)\n",
        "poly_lin_reg_r2 = r2_score(y_test, poly_y_pred_lin)\n",
        "train_poly_lin_reg_mse = mean_squared_error(y_train, poly_lin_reg.predict(X_train))\n",
        "\n",
        "#svr\n",
        "poly_svr_mse = mean_squared_error(y_test, poly_y_pred_svr)\n",
        "poly_svr_r2 = r2_score(y_test, poly_y_pred_svr)\n",
        "train_poly_svr_mse = mean_squared_error(y_train, poly_svr.predict(X_train))\n",
        "\n",
        "#xgb\n",
        "poly_xgb_mse = mean_squared_error(y_test, poly_y_pred_xgb)\n",
        "poly_xgb_r2 = r2_score(y_test, poly_y_pred_xgb)\n",
        "train_poly_xgb_mse = mean_squared_error(y_train, poly_xgb.predict(X_train))\n",
        "\n",
        "print(\"TRAINING VALIDATION\")\n",
        "print(f\"Linear Regression Training MSE: {train_poly_lin_reg_mse}\")\n",
        "print(f\"SVR Training MSE: {train_poly_svr_mse}\")\n",
        "print(f\"XGBoost Training MSE: {train_poly_xgb_mse}\")\n",
        "\n",
        "\n",
        "print(\"TESTING VALIDATION\")\n",
        "print(f\"Linear Regression MSE: {poly_lin_reg_mse}\")\n",
        "print(f\"Linear Regression R^2: {poly_lin_reg_r2}\")\n",
        "print(f\"SVR MSE: {poly_svr_mse}\")\n",
        "print(f\"SVR R^2: {poly_svr_r2}\")\n",
        "print(f\"XGBoost MSE: {poly_xgb_mse}\")\n",
        "print(f\"XGBoost R^2: {poly_xgb_r2}\")\n",
        "\n",
        "print(f\"Poly Linear Regression Training Time: {poly_lin_train_time} seconds\")\n",
        "print(f\"Poly SVR Training Time: {poly_svr_train_time} seconds\")\n",
        "print(f\"Poly XGBoost Training Time: {poly_xgb_train_time} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCt9dkVIXAB6",
        "outputId": "8f3a41f0-8ca7-445e-bdc0-fd4c4b4e54bd"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING VALIDATION\n",
            "Linear Regression Training MSE: 6.214375040394961e-29\n",
            "SVR Training MSE: 0.9692832948752294\n",
            "XGBoost Training MSE: 0.811391216941027\n",
            "TESTING VALIDATION\n",
            "Linear Regression MSE: 15.811879945458458\n",
            "Linear Regression R^2: 0.11947404600794098\n",
            "SVR MSE: 7.388037075403568\n",
            "SVR R^2: 0.5885778024885069\n",
            "XGBoost MSE: 5.490413432665133\n",
            "XGBoost R^2: 0.6942519458606913\n",
            "Poly Linear Regression Training Time: 0.5081236362457275 seconds\n",
            "Poly SVR Training Time: 1.975726842880249 seconds\n",
            "Poly XGBoost Training Time: 5.559717416763306 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"OBSERVATION:\n",
        "Introducing a polynomial feature appears to significantly worsen the performance\n",
        "of the linear regression model - this is expected since the purpose is to\n",
        "introduce non-linearity to the data. This results in linear regression highly overfitting the train data, creating a too complex model\n",
        "especially when there is a small sample size of 40 combined with 152 features created by the polynomial transformation.\n",
        "\n",
        "Polynomial features can introduce high multicollinearity (strong correlation among the features).\n",
        "This can make it difficult for linear regression to estimate the\n",
        "coefficients accurately and can lead to worse predictions.\n",
        "\n",
        "For 2nd order polynomials,\n",
        "XGBoost performed the best out of the three models.\n",
        "This may be because XGBoost captures more complex relationships within the data\n",
        "better than simple linear regressions, showing its superiority in high-dimensional data.\n",
        "\n",
        "Choosing 3rd order polynomial however, significantly improved SVR testing error,\n",
        "making it the best model, while XGBoost showed signs of overfitting.\n",
        "\n",
        "2nd order results:\n",
        "TRAINING VALIDATION\n",
        "Linear Regression Training MSE: 6.214375040394961e-29\n",
        "SVR Training MSE: 0.9692832948752294\n",
        "XGBoost Training MSE: 0.811391216941027\n",
        "TESTING VALIDATION\n",
        "Linear Regression MSE: 15.811879945458458\n",
        "Linear Regression R^2: 0.11947404600794098\n",
        "SVR MSE: 7.388037075403568\n",
        "SVR R^2: 0.5885778024885069\n",
        "XGBoost MSE: 5.490413432665133\n",
        "XGBoost R^2: 0.6942519458606913\n",
        "Poly Linear Regression Training Time: 0.40543150901794434 seconds\n",
        "Poly SVR Training Time: 1.9447619915008545 seconds\n",
        "Poly XGBoost Training Time: 5.509463310241699 seconds\n",
        "\n",
        "3rd order results\n",
        "TRAINING VALIDATION\n",
        "Linear Regression Training MSE: 1.0319903014004565e-28\n",
        "SVR Training MSE: 1.0182558407620799\n",
        "XGBoost Training MSE: 0.5436830734271374\n",
        "TESTING VALIDATION\n",
        "Linear Regression MSE: 16.242248176341487\n",
        "Linear Regression R^2: 0.0955078637213751\n",
        "SVR MSE: 6.297163643151013\n",
        "SVR R^2: 0.6493259471071144\n",
        "XGBoost MSE: 6.934081755189215\n",
        "XGBoost R^2: 0.6138574936308825\n",
        "Poly Linear Regression Training Time: 0.4049391746520996 seconds\n",
        "Poly SVR Training Time: 2.2321090698242188 seconds\n",
        "Poly XGBoost Training Time: 25.893834590911865 seconds\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlcBKJvIXwOJ",
        "outputId": "af2ea571-15a4-415f-f379-fab7a600d25e"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'OBSERVATION:\\nIntroducing a polynomial feature appears to significantly worsen the performance\\nof the linear regression model - this is expected since the purpose is to\\nintroduce non-linearity to the data. This results in linear regression highly overfitting the train data, creating a too complex model\\nespecially when there is a small sample size of 40 combined with 152 features created by the polynomial transformation.\\n\\nPolynomial features can introduce high multicollinearity (strong correlation among the features).\\nThis can make it difficult for linear regression to estimate the\\ncoefficients accurately and can lead to worse predictions.\\n\\nFor 2nd order polynomials,\\nXGBoost performed the best out of the three models.\\nThis may be because XGBoost captures more complex relationships within the data\\nbetter than simple linear regressions, showing its superiority in high-dimensional data.\\n\\nChoosing 3rd order polynomial however, significantly improved SVR testing error,\\nmaking it the best model, while XGBoost showed signs of overfitting.\\n\\n2nd order results:\\nTRAINING VALIDATION\\nLinear Regression Training MSE: 6.214375040394961e-29\\nSVR Training MSE: 0.9692832948752294\\nXGBoost Training MSE: 0.811391216941027\\nTESTING VALIDATION\\nLinear Regression MSE: 15.811879945458458\\nLinear Regression R^2: 0.11947404600794098\\nSVR MSE: 7.388037075403568\\nSVR R^2: 0.5885778024885069\\nXGBoost MSE: 5.490413432665133\\nXGBoost R^2: 0.6942519458606913\\nPoly Linear Regression Training Time: 0.40543150901794434 seconds\\nPoly SVR Training Time: 1.9447619915008545 seconds\\nPoly XGBoost Training Time: 5.509463310241699 seconds\\n\\n3rd order results\\nTRAINING VALIDATION\\nLinear Regression Training MSE: 1.0319903014004565e-28\\nSVR Training MSE: 1.0182558407620799\\nXGBoost Training MSE: 0.5436830734271374\\nTESTING VALIDATION\\nLinear Regression MSE: 16.242248176341487\\nLinear Regression R^2: 0.0955078637213751\\nSVR MSE: 6.297163643151013\\nSVR R^2: 0.6493259471071144\\nXGBoost MSE: 6.934081755189215\\nXGBoost R^2: 0.6138574936308825\\nPoly Linear Regression Training Time: 0.4049391746520996 seconds\\nPoly SVR Training Time: 2.2321090698242188 seconds\\nPoly XGBoost Training Time: 25.893834590911865 seconds\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EXPERIMENT 3: FEATURE TRANSFORMATION"
      ],
      "metadata": {
        "id": "wlLVy7rPDMHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#USE PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
        "\n",
        "#apply scaling\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "#apply pca\n",
        "pca = PCA(n_components=0.9)\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "#lin reg\n",
        "pca_lin_reg = LinearRegression()\n",
        "\n",
        "t0 = time.time()\n",
        "%memit pca_lin_reg.fit(X_train, y_train)\n",
        "t1 = time.time()\n",
        "pca_lin_train_time = t1 - t0\n",
        "\n",
        "t0 = time.time()\n",
        "pca_y_pred_lin = pca_lin_reg.predict(X_test)\n",
        "t1 = time.time()\n",
        "pca_lin_infer_time = t1 - t0\n",
        "\n",
        "#SVR\n",
        "grid_search = GridSearchCV(SVR(), svr_param_grid, cv=5, n_jobs=-1, verbose=True)\n",
        "\n",
        "t0 = time.time()\n",
        "%memit grid_search.fit(X_train, y_train)\n",
        "t1 = time.time()\n",
        "pca_svr_train_time = t1 - t0\n",
        "\n",
        "pca_svr = grid_search.best_estimator_\n",
        "\n",
        "t0 = time.time()\n",
        "pca_y_pred_svr = pca_svr.predict(X_test)\n",
        "t1 = time.time()\n",
        "pca_svr_infer_time = t1 - t0\n",
        "\n",
        "print(f\"Best SVR parameters: %s\" % grid_search.best_params_)\n",
        "\n",
        "#XGBoost\n",
        "\n",
        "#Best XGBoost parameters:{'colsample_bytree': 0.75, 'eta': 0.3, 'gamma': 1, 'max_depth': 5, 'n_estimators': 300, 'n_jobs': -1, 'reg_alpha': 3, 'reg_lambda': 0.5, 'subsample': 1}\n",
        "\n",
        "grid_search = GridSearchCV(xgb.XGBRegressor(), XG_param_grid, cv=5, n_jobs=-1, scoring = \"neg_root_mean_squared_error\", verbose=True)\n",
        "\n",
        "t0 = time.time()\n",
        "%memit grid_search.fit(X_train, y_train)\n",
        "t1 = time.time()\n",
        "pca_xgb_train_time = t1 - t0\n",
        "\n",
        "pca_xgb = grid_search.best_estimator_\n",
        "\n",
        "t0 = time.time()\n",
        "pca_y_pred_xgb = pca_xgb.predict(X_test)\n",
        "t1 = time.time()\n",
        "pca_xgb_infer_time = t1 - t0\n",
        "\n",
        "print(f\"Best XGBoost parameters:%s\" % grid_search.best_params_)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UheHHhIWJk2y",
        "outputId": "8d7a983b-22b2-479f-afeb-8cd98388a2e7"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "peak memory: 522.05 MiB, increment: 0.00 MiB\n",
            "Fitting 5 folds for each of 1500 candidates, totalling 7500 fits\n",
            "peak memory: 522.05 MiB, increment: 0.00 MiB\n",
            "Best SVR parameters: {'C': 4.5, 'degree': 3, 'epsilon': 0.1, 'kernel': 'poly'}\n",
            "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
            "peak memory: 522.05 MiB, increment: 0.00 MiB\n",
            "Best XGBoost parameters:{'colsample_bytree': 0.75, 'eta': 0.3, 'gamma': 0, 'max_depth': 5, 'n_estimators': 300, 'n_jobs': -1, 'reg_alpha': 3, 'reg_lambda': 1, 'subsample': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate\n",
        "#lin reg\n",
        "pca_lin_reg_mse = mean_squared_error(y_test, pca_y_pred_lin)\n",
        "pca_lin_reg_r2 = r2_score(y_test, pca_y_pred_lin)\n",
        "train_pca_lin_reg_mse = mean_squared_error(y_train, pca_lin_reg.predict(X_train))\n",
        "\n",
        "#svr\n",
        "pca_svr_mse = mean_squared_error(y_test, pca_y_pred_svr)\n",
        "pca_svr_r2 = r2_score(y_test, pca_y_pred_svr)\n",
        "train_pca_svr_mse = mean_squared_error(y_train, pca_svr.predict(X_train))\n",
        "\n",
        "#xgb\n",
        "pca_xgb_mse = mean_squared_error(y_test, pca_y_pred_xgb)\n",
        "pca_xgb_r2 = r2_score(y_test, pca_y_pred_xgb)\n",
        "train_pca_xgb_mse = mean_squared_error(y_train, pca_xgb.predict(X_train))\n",
        "\n",
        "print(\"TRAINING VALIDATION\")\n",
        "print(f\"Linear Regression Training MSE: {train_pca_lin_reg_mse}\")\n",
        "print(f\"SVR Training MSE: {train_pca_svr_mse}\")\n",
        "print(f\"XGBoost Training MSE: {train_pca_xgb_mse}\")\n",
        "\n",
        "print(\"TESTING VALIDATION\")\n",
        "print(f\"Linear Regression MSE: {pca_lin_reg_mse}\")\n",
        "print(f\"Linear Regression R^2: {pca_lin_reg_r2}\")\n",
        "print(f\"SVR MSE: {pca_svr_mse}\")\n",
        "print(f\"SVR R^2: {pca_svr_r2}\")\n",
        "print(f\"XGBoost MSE: {pca_xgb_mse}\")\n",
        "print(f\"XGBoost R^2: {pca_xgb_r2}\")\n",
        "\n",
        "print(f\"PCA Linear Regression Training Time: {pca_lin_train_time} seconds\")\n",
        "print(f\"PCA SVR Training Time: {pca_svr_train_time} seconds\")\n",
        "print(f\"PCA XGBoost Training Time: {pca_xgb_train_time} seconds\")\n",
        "\n",
        "print(f\"PCA reduced to {pca.n_components_} components! \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQJBO4trc_6V",
        "outputId": "6236a91f-3298-431a-ff90-1c9f6225f43d"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING VALIDATION\n",
            "Linear Regression Training MSE: 3.3223205107630847\n",
            "SVR Training MSE: 1.8612041339912593\n",
            "XGBoost Training MSE: 0.30955425951736826\n",
            "TESTING VALIDATION\n",
            "Linear Regression MSE: 5.839949811238412\n",
            "Linear Regression R^2: 0.6747870970090805\n",
            "SVR MSE: 7.21511477037949\n",
            "SVR R^2: 0.5982074340138566\n",
            "XGBoost MSE: 5.586542281781182\n",
            "XGBoost R^2: 0.688898759088085\n",
            "PCA Linear Regression Training Time: 0.5001959800720215 seconds\n",
            "PCA SVR Training Time: 1.9499282836914062 seconds\n",
            "PCA XGBoost Training Time: 2.503145694732666 seconds\n",
            "PCA reduced to 6 components! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"OBSERVATION:\n",
        "Reducing PCA to capture 90% of variance reduced the features to 6 components (out of 18).\n",
        "However, this also appears to raise MSE for XGBoost, while lowering SVR, and Linear regression.\n",
        "Further experimentation with XGBoost parameters may be needed.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMVJwu5cgDvV",
        "outputId": "d75e6b94-6b57-4ab1-ce29-24ebf73ae17f"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'OBSERVATION:\\nReducing PCA to capture 90% of variance reduced the features to 6 components (out of 18).\\nHowever, this also appears to raise MSE for XGBoost, while lowering SVR, and Linear regression.\\nFurther experimentation with XGBoost parameters may be needed.\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EXPERIMENT 4: HANDLING NOISE\n",
        "Introduce synthetic noise:\n",
        "\n",
        "• Add a random continuous feature (you can use random.Generator.uniform)\n",
        "\n",
        "• Add a random discrete categorical feature"
      ],
      "metadata": {
        "id": "lvl-fdBpjvEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Introduce synthetic noise:\n",
        "#Add a random continuous feature using random.Generator.uniform\n",
        "\n",
        "noise = np.random.rand(len(data))\n",
        "\n",
        "#random discrete categorical feature, 2 classes\n",
        "random_categories = np.random.randint(0, 2, size=len(data))\n",
        "data['RANDOM_CAT'] = random_categories\n",
        "data['NOISE'] = noise\n",
        "#display(data)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
        "\n",
        "#apply scaling\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "#lin reg\n",
        "noise_lin_reg = LinearRegression()\n",
        "\n",
        "t0 = time.time()\n",
        "%memit noise_lin_reg.fit(X_train, y_train)\n",
        "t1 = time.time()\n",
        "noise_lin_train_time = t1 - t0\n",
        "\n",
        "t0 = time.time()\n",
        "noise_y_pred_lin = noise_lin_reg.predict(X_test)\n",
        "t1 = time.time()\n",
        "noise_lin_infer_time = t1 - t0\n",
        "\n",
        "#SVR\n",
        "grid_search = GridSearchCV(SVR(), svr_param_grid, cv=5, n_jobs=-1, verbose=True)\n",
        "\n",
        "t0 = time.time()\n",
        "%memit grid_search.fit(X_train, y_train)\n",
        "t1 = time.time()\n",
        "noise_svr_train_time = t1 - t0\n",
        "\n",
        "noise_svr = grid_search.best_estimator_\n",
        "\n",
        "t0 = time.time()\n",
        "noise_y_pred_svr = noise_svr.predict(X_test)\n",
        "t1 = time.time()\n",
        "noise_svr_infer_time = t1 - t0\n",
        "\n",
        "print(f\"Best SVR parameters: %s\" % grid_search.best_params_)\n",
        "\n",
        "#XGBoost\n",
        "\n",
        "#Best XGBoost parameters:{'colsample_bytree': 0.75, 'eta': 0.3, 'gamma': 1, 'max_depth': 5, 'n_estimators': 300, 'n_jobs': -1, 'reg_alpha': 3, 'reg_lambda': 0.5, 'subsample': 1}\n",
        "\n",
        "grid_search = GridSearchCV(xgb.XGBRegressor(), XG_param_grid, cv=5, n_jobs=-1, scoring = \"neg_root_mean_squared_error\", verbose=True)\n",
        "\n",
        "t0 = time.time()\n",
        "%memit grid_search.fit(X_train, y_train)\n",
        "t1 = time.time()\n",
        "noise_xgb_train_time = t1 - t0\n",
        "\n",
        "noise_xgb = grid_search.best_estimator_\n",
        "\n",
        "t0 = time.time()\n",
        "noise_y_pred_xgb = noise_xgb.predict(X_test)\n",
        "t1 = time.time()\n",
        "noise_xgb_infer_time = t1 - t0\n",
        "\n",
        "print(f\"Best XGBoost parameters:%s\" % grid_search.best_params_)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xO32G0Dj96c",
        "outputId": "b217e008-0788-435a-956c-358ee20a939e"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "peak memory: 522.05 MiB, increment: 0.00 MiB\n",
            "Fitting 5 folds for each of 1500 candidates, totalling 7500 fits\n",
            "peak memory: 522.05 MiB, increment: 0.00 MiB\n",
            "Best SVR parameters: {'C': 3.4000000000000004, 'degree': 2, 'epsilon': 0.001, 'kernel': 'linear'}\n",
            "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
            "peak memory: 522.05 MiB, increment: 0.00 MiB\n",
            "Best XGBoost parameters:{'colsample_bytree': 0.75, 'eta': 0.3, 'gamma': 1, 'max_depth': 8, 'n_estimators': 300, 'n_jobs': -1, 'reg_alpha': 3, 'reg_lambda': 1, 'subsample': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate\n",
        "#lin reg\n",
        "noise_lin_reg_mse = mean_squared_error(y_test, noise_y_pred_lin)\n",
        "noise_lin_reg_r2 = r2_score(y_test, noise_y_pred_lin)\n",
        "train_noise_lin_reg_mse = mean_squared_error(y_train, noise_lin_reg.predict(X_train))\n",
        "\n",
        "#svr\n",
        "noise_svr_mse = mean_squared_error(y_test, noise_y_pred_svr)\n",
        "noise_svr_r2 = r2_score(y_test, noise_y_pred_svr)\n",
        "train_noise_svr_mse = mean_squared_error(y_train, noise_svr.predict(X_train))\n",
        "\n",
        "#xgb\n",
        "noise_xgb_mse = mean_squared_error(y_test, noise_y_pred_xgb)\n",
        "noise_xgb_r2 = r2_score(y_test, noise_y_pred_xgb)\n",
        "train_noise_xgb_mse = mean_squared_error(y_train, noise_xgb.predict(X_train))\n",
        "\n",
        "print(\"TRAINING VALIDATION\")\n",
        "print(f\"Linear Regression Training MSE: {train_noise_lin_reg_mse}\")\n",
        "print(f\"SVR Training MSE: {train_noise_svr_mse}\")\n",
        "print(f\"XGBoost Training MSE: {train_noise_xgb_mse}\")\n",
        "\n",
        "print(\"TESTING VALIDATION\")\n",
        "print(f\"Linear Regression MSE: {noise_lin_reg_mse}\")\n",
        "print(f\"Linear Regression R^2: {noise_lin_reg_r2}\")\n",
        "print(f\"SVR MSE: {noise_svr_mse}\")\n",
        "print(f\"SVR R^2: {noise_svr_r2}\")\n",
        "print(f\"XGBoost MSE: {noise_xgb_mse}\")\n",
        "print(f\"XGBoost R^2: {noise_xgb_r2}\")\n",
        "\n",
        "print(f\"Noisy Linear Regression Training Time: {noise_lin_train_time} seconds\")\n",
        "print(f\"Noisy SVR Training Time: {noise_svr_train_time} seconds\")\n",
        "print(f\"Noisy XGBoost Training Time: {noise_xgb_train_time} seconds\")\n",
        "\n",
        "#SVR appears most robust to noisy data!\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2vMOcaEm1rS",
        "outputId": "333fbca8-073a-4520-9c28-f59f2764ed9e"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING VALIDATION\n",
            "Linear Regression Training MSE: 1.6413816054918349\n",
            "SVR Training MSE: 5.233692401214437\n",
            "XGBoost Training MSE: 0.7895197633356645\n",
            "TESTING VALIDATION\n",
            "Linear Regression MSE: 7.959046258848479\n",
            "Linear Regression R^2: 0.556779660349471\n",
            "SVR MSE: 5.617370160145802\n",
            "SVR R^2: 0.6871820279277049\n",
            "XGBoost MSE: 7.449110418871594\n",
            "XGBoost R^2: 0.5851767733758291\n",
            "Noisy Linear Regression Training Time: 0.46941590309143066 seconds\n",
            "Noisy SVR Training Time: 1.8483541011810303 seconds\n",
            "Noisy XGBoost Training Time: 2.725550413131714 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Efficiency"
      ],
      "metadata": {
        "id": "rIzqyTi5n6gL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TIME COMPARISONS\n",
        "time_df = pd.DataFrame(columns=['Model Variant', 'Training Time', 'Inference Time'])\n",
        "\n",
        "#Linear Regression\n",
        "time_df.loc[len(time_df)] = ['BASE Linear Regression', base_lin_train_time, base_lin_infer_time]\n",
        "time_df.loc[len(time_df)] = ['SCALED Linear Regression', scale_lin_train_time, scale_lin_infer_time]\n",
        "time_df.loc[len(time_df)] = ['POLY Linear Regression', poly_lin_train_time, poly_lin_infer_time]\n",
        "time_df.loc[len(time_df)] = ['PCA Linear Regression', pca_lin_train_time, pca_lin_infer_time]\n",
        "time_df.loc[len(time_df)] = ['NOISY Linear Regression', noise_lin_train_time, noise_lin_infer_time]\n",
        "\n",
        "#SVR\n",
        "time_df.loc[len(time_df)] = ['BASE SVR', base_svr_train_time, base_svr_infer_time]\n",
        "time_df.loc[len(time_df)] = ['SCALED SVR', scale_svr_train_time, scale_svr_infer_time]\n",
        "time_df.loc[len(time_df)] = ['POLY SVR', poly_svr_train_time, poly_svr_infer_time]\n",
        "time_df.loc[len(time_df)] = ['PCA SVR', pca_svr_train_time, pca_svr_infer_time]\n",
        "time_df.loc[len(time_df)] = ['NOISY SVR', noise_svr_train_time, noise_svr_infer_time]\n",
        "\n",
        "#XGBoost\n",
        "time_df.loc[len(time_df)] = ['BASE XGBoost', base_xgb_train_time, base_xgb_infer_time]\n",
        "time_df.loc[len(time_df)] = ['SCALED XGBoost', scale_xgb_train_time, scale_xgb_infer_time]\n",
        "time_df.loc[len(time_df)] = ['POLY XGBoost', poly_xgb_train_time, poly_xgb_infer_time]\n",
        "time_df.loc[len(time_df)] = ['PCA XGBoost', pca_xgb_train_time, pca_xgb_infer_time]\n",
        "time_df.loc[len(time_df)] = ['NOISY XGBoost', noise_xgb_train_time, noise_xgb_infer_time]\n",
        "\n",
        "display(time_df)\n",
        "\n",
        "\"\"\"\n",
        "If required to deploy a model, which trade-offs are necessary?\n",
        "\n",
        "If speed is important: Linear regression offers the fastest speeds, and decent\n",
        "accuracy for this dataset (except for when adding Polynomial features!).\n",
        "\n",
        "If accuracy is important: SVR outperformed linear and XGBoost in some cases,\n",
        "although linear had good accuracy as well, along with its speed.\n",
        "XGBoost requires much more hyperparameter experimentation to be considered as the best.\n",
        "\n",
        "If interpretability is important: Linear Regression is the preferred choice,\n",
        "as it offers a clearer understanding of feature importance and relationships.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "eLuYnTJS1yCe",
        "outputId": "6b63d164-a868-49d7-d6c8-f922abb4f98c"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "               Model Variant  Training Time  Inference Time\n",
              "0     BASE Linear Regression       0.466213        0.000973\n",
              "1   SCALED Linear Regression       0.467712        0.000452\n",
              "2     POLY Linear Regression       0.508124        0.000346\n",
              "3      PCA Linear Regression       0.500196        0.000287\n",
              "4    NOISY Linear Regression       0.469416        0.000291\n",
              "5                   BASE SVR       4.946299        0.001350\n",
              "6                 SCALED SVR       1.861369        0.000886\n",
              "7                   POLY SVR       1.975727        0.000829\n",
              "8                    PCA SVR       1.949928        0.000865\n",
              "9                  NOISY SVR       1.848354        0.000809\n",
              "10              BASE XGBoost       2.923882        0.001548\n",
              "11            SCALED XGBoost       2.731527        0.000857\n",
              "12              POLY XGBoost       5.559717        0.013268\n",
              "13               PCA XGBoost       2.503146        0.003664\n",
              "14             NOISY XGBoost       2.725550        0.000886"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model Variant</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Inference Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BASE Linear Regression</td>\n",
              "      <td>0.466213</td>\n",
              "      <td>0.000973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SCALED Linear Regression</td>\n",
              "      <td>0.467712</td>\n",
              "      <td>0.000452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>POLY Linear Regression</td>\n",
              "      <td>0.508124</td>\n",
              "      <td>0.000346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PCA Linear Regression</td>\n",
              "      <td>0.500196</td>\n",
              "      <td>0.000287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NOISY Linear Regression</td>\n",
              "      <td>0.469416</td>\n",
              "      <td>0.000291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>BASE SVR</td>\n",
              "      <td>4.946299</td>\n",
              "      <td>0.001350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>SCALED SVR</td>\n",
              "      <td>1.861369</td>\n",
              "      <td>0.000886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>POLY SVR</td>\n",
              "      <td>1.975727</td>\n",
              "      <td>0.000829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>PCA SVR</td>\n",
              "      <td>1.949928</td>\n",
              "      <td>0.000865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NOISY SVR</td>\n",
              "      <td>1.848354</td>\n",
              "      <td>0.000809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>BASE XGBoost</td>\n",
              "      <td>2.923882</td>\n",
              "      <td>0.001548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>SCALED XGBoost</td>\n",
              "      <td>2.731527</td>\n",
              "      <td>0.000857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>POLY XGBoost</td>\n",
              "      <td>5.559717</td>\n",
              "      <td>0.013268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>PCA XGBoost</td>\n",
              "      <td>2.503146</td>\n",
              "      <td>0.003664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>NOISY XGBoost</td>\n",
              "      <td>2.725550</td>\n",
              "      <td>0.000886</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIf required to deploy a model, which trade-offs are necessary?\\n\\nIf speed is important: Linear regression offers the fastest speeds, and decent\\naccuracy for this dataset (except for when adding Polynomial features!).\\n\\nIf accuracy is important: SVR outperformed linear and XGBoost in some cases,\\nalthough linear had good accuracy as well, along with its speed.\\nXGBoost requires much more hyperparameter experimentation to be considered as the best.\\n\\nIf interpretability is important: Linear Regression is the preferred choice,\\nas it offers a clearer understanding of feature importance and relationships.\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Summarization of findings\n"
      ],
      "metadata": {
        "id": "jv8JW08B54BE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#SUMMARIZATION TABLE\n",
        "mse_df = pd.DataFrame(columns=['Experiment','Model Variant', 'Train MSE', 'Test MSE'])\n",
        "#linear reg errors\n",
        "mse_df.loc[len(mse_df)] = ['BASE','Linear Regression', train_lin_reg_mse, lin_reg_mse]\n",
        "mse_df.loc[len(mse_df)] = ['SCALED', 'Linear Regression', train_scale_lin_reg_mse, scale_lin_reg_mse]\n",
        "mse_df.loc[len(mse_df)] = ['POLY', 'Linear Regression', train_poly_lin_reg_mse, poly_lin_reg_mse]\n",
        "mse_df.loc[len(mse_df)] = ['PCA', 'Linear Regression', train_pca_lin_reg_mse, pca_lin_reg_mse]\n",
        "mse_df.loc[len(mse_df)] = ['NOISY', 'Linear Regression', train_noise_lin_reg_mse, noise_lin_reg_mse]\n",
        "\n",
        "# SVR\n",
        "mse_df.loc[len(mse_df)] = ['BASE', 'SVR', train_svr_mse, svr_mse]\n",
        "mse_df.loc[len(mse_df)] = ['SCALED', 'SVR', train_scale_svr_mse, scale_svr_mse]\n",
        "mse_df.loc[len(mse_df)] = ['POLY','SVR', train_poly_svr_mse, poly_svr_mse]\n",
        "mse_df.loc[len(mse_df)] = ['PCA', 'SVR', train_pca_svr_mse, pca_svr_mse]\n",
        "mse_df.loc[len(mse_df)] = ['NOISY', 'SVR', train_noise_svr_mse, noise_svr_mse]\n",
        "\n",
        "# XGBoost\n",
        "mse_df.loc[len(mse_df)] = ['BASE', 'XGBoost', train_xgb_mse, xgb_mse]\n",
        "mse_df.loc[len(mse_df)] = ['SCALED', 'XGBoost', train_scale_xgb_mse, scale_xgb_mse]\n",
        "mse_df.loc[len(mse_df)] = ['POLY', 'XGBoost', train_poly_xgb_mse, poly_xgb_mse]\n",
        "mse_df.loc[len(mse_df)] = ['PCA', 'XGBoost', train_pca_xgb_mse, pca_xgb_mse]\n",
        "mse_df.loc[len(mse_df)] = ['NOISY', 'XGBoost', train_noise_xgb_mse, noise_xgb_mse]\n",
        "\n",
        "\n",
        "#display, ordered by experiemnt\n",
        "display(mse_df.sort_values(by=['Experiment']))\n",
        "\n",
        "#display only base\n",
        "#display(mse_df[mse_df['Experiment'] == 'BASE'])\n",
        "\n",
        "print(f\"Lowest Train MSE: {mse_df.loc[mse_df['Train MSE'].idxmin()]['Train MSE']} {mse_df.loc[mse_df['Train MSE'].idxmin()]['Model Variant']} {mse_df.loc[mse_df['Train MSE'].idxmin()]['Experiment']}\")\n",
        "print(f\"Highest Train MSE: {mse_df.loc[mse_df['Train MSE'].idxmax()]['Train MSE']} {mse_df.loc[mse_df['Train MSE'].idxmax()]['Model Variant']} {mse_df.loc[mse_df['Train MSE'].idxmax()]['Experiment']}\")\n",
        "\n",
        "print(f\"Lowest Test MSE: {mse_df.loc[mse_df['Test MSE'].idxmin()]['Test MSE']} {mse_df.loc[mse_df['Test MSE'].idxmin()]['Model Variant']} {mse_df.loc[mse_df['Test MSE'].idxmin()]['Experiment']}\")\n",
        "print(f\"Highest Test MSE: {mse_df.loc[mse_df['Test MSE'].idxmax()]['Test MSE']} {mse_df.loc[mse_df['Test MSE'].idxmax()]['Model Variant']} {mse_df.loc[mse_df['Test MSE'].idxmax()]['Experiment']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "8hXCUjzi58Je",
        "outputId": "3a4ea8bb-ad59-4932-e346-08bcbe2e8764"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Experiment      Model Variant     Train MSE   Test MSE\n",
              "0        BASE  Linear Regression  1.857933e+00   6.021257\n",
              "5        BASE                SVR  3.377912e+00   6.813620\n",
              "10       BASE            XGBoost  9.878889e-01   5.910050\n",
              "4       NOISY  Linear Regression  1.641382e+00   7.959046\n",
              "9       NOISY                SVR  5.233692e+00   5.617370\n",
              "14      NOISY            XGBoost  7.895198e-01   7.449110\n",
              "3         PCA  Linear Regression  3.322321e+00   5.839950\n",
              "8         PCA                SVR  1.861204e+00   7.215115\n",
              "13        PCA            XGBoost  3.095543e-01   5.586542\n",
              "2        POLY  Linear Regression  6.214375e-29  15.811880\n",
              "7        POLY                SVR  9.692833e-01   7.388037\n",
              "12       POLY            XGBoost  8.113912e-01   5.490413\n",
              "1      SCALED  Linear Regression  1.857933e+00   6.021257\n",
              "6      SCALED                SVR  6.436058e+00   5.340416\n",
              "11     SCALED            XGBoost  9.878889e-01   5.910050"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Experiment</th>\n",
              "      <th>Model Variant</th>\n",
              "      <th>Train MSE</th>\n",
              "      <th>Test MSE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BASE</td>\n",
              "      <td>Linear Regression</td>\n",
              "      <td>1.857933e+00</td>\n",
              "      <td>6.021257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>BASE</td>\n",
              "      <td>SVR</td>\n",
              "      <td>3.377912e+00</td>\n",
              "      <td>6.813620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>BASE</td>\n",
              "      <td>XGBoost</td>\n",
              "      <td>9.878889e-01</td>\n",
              "      <td>5.910050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NOISY</td>\n",
              "      <td>Linear Regression</td>\n",
              "      <td>1.641382e+00</td>\n",
              "      <td>7.959046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NOISY</td>\n",
              "      <td>SVR</td>\n",
              "      <td>5.233692e+00</td>\n",
              "      <td>5.617370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>NOISY</td>\n",
              "      <td>XGBoost</td>\n",
              "      <td>7.895198e-01</td>\n",
              "      <td>7.449110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PCA</td>\n",
              "      <td>Linear Regression</td>\n",
              "      <td>3.322321e+00</td>\n",
              "      <td>5.839950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>PCA</td>\n",
              "      <td>SVR</td>\n",
              "      <td>1.861204e+00</td>\n",
              "      <td>7.215115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>PCA</td>\n",
              "      <td>XGBoost</td>\n",
              "      <td>3.095543e-01</td>\n",
              "      <td>5.586542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>POLY</td>\n",
              "      <td>Linear Regression</td>\n",
              "      <td>6.214375e-29</td>\n",
              "      <td>15.811880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>POLY</td>\n",
              "      <td>SVR</td>\n",
              "      <td>9.692833e-01</td>\n",
              "      <td>7.388037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>POLY</td>\n",
              "      <td>XGBoost</td>\n",
              "      <td>8.113912e-01</td>\n",
              "      <td>5.490413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SCALED</td>\n",
              "      <td>Linear Regression</td>\n",
              "      <td>1.857933e+00</td>\n",
              "      <td>6.021257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>SCALED</td>\n",
              "      <td>SVR</td>\n",
              "      <td>6.436058e+00</td>\n",
              "      <td>5.340416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>SCALED</td>\n",
              "      <td>XGBoost</td>\n",
              "      <td>9.878889e-01</td>\n",
              "      <td>5.910050</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lowest Train MSE: 6.214375040394961e-29 Linear Regression POLY\n",
            "Highest Train MSE: 6.436057875193771 SVR SCALED\n",
            "Lowest Test MSE: 5.340416141768769 SVR SCALED\n",
            "Highest Test MSE: 15.811879945458458 Linear Regression POLY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Final model choice\n",
        "\n",
        "It appears that SVR with a linear kernal on scaled features achieved the lowest MSE compared to other model variants - therefore, this is my recomended model for the given dataset. This model effectively captured the general pattern within the dataset without excessive overfitting, leading to improved predictive performance.\n",
        "\n",
        "The fact that only scaled data was required to achieve the best results suggests that the dataset exhibits a linear relationship between the features and the target. Simple linear models, like SVR with a linear kernel, is sufficient to capture the underlying patterns, while XGBoost likely overfit the data. More complex feature transformations does not appear provide significant improvements.\n",
        "\n",
        "##Proposed Further Refinements\n",
        "If more time, and patience, were available, a more extensive GridSearch over a wider range of values could potentially yield better results, especially for XGBoost which tended to overfit the data. Using techniques like RandomizedSearchCV or Bayesian Optimization would alternatively allow for a more efficient exploration of the hyperparameter space.\n",
        "\n",
        "Also, due to only 50 samples (50 states), it may be better to incorporate more data from additional years, or elect to analyze county data.\n",
        "\n",
        "Experimenting with other models, particularly  Ensemble Methods could potentially improve overall accuracy and stability.\n",
        "\n"
      ],
      "metadata": {
        "id": "rkmomlHP78CG"
      }
    }
  ]
}